{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "texts = rag_df[\"text\"].astype(str).tolist()\n",
        "emb = embed_model.encode(texts, batch_size=64, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)\n",
        "\n",
        "index = faiss.IndexFlatIP(emb.shape[1])\n",
        "index.add(emb)\n",
        "print(\"FAISS vectors:\", index.ntotal)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "secondary'den pozitif √∂rnekler (monitor/avoid)\n",
        "\n",
        "df_pos = df_secondary[df_secondary[\"action_3class\"].isin([\"monitor\",\"avoid\"])][[\"drugA_norm\",\"drugB_norm\",\"pair_key\",\"action_3class\"]].dropna().drop_duplicates()\n",
        "\n",
        "negatif √ºretim: random drug havuzundan \"hi√ßbir kaynakta g√∂r√ºnmeyen\" √ßiftler se√ß\n",
        "\n",
        "all_drugs = sorted(set(df_secondary[\"drugA_norm\"].dropna().tolist() + df_secondary[\"drugB_norm\"].dropna().tolist()))\n",
        "pos_keys = set(df_pos[\"pair_key\"].tolist())\n",
        "\n",
        "def generate_no_interaction_pairs(n=3000, max_tries=200000):\n",
        "rng = np.random.default_rng(42)\n",
        "out = []\n",
        "tries = 0\n",
        "out_keys = set()\n",
        "\n",
        "while len(out) < n and tries < max_tries:\n",
        "    tries += 1\n",
        "    a = all_drugs[int(rng.integers(0, len(all_drugs)))]\n",
        "    b = all_drugs[int(rng.integers(0, len(all_drugs)))]\n",
        "    if a == b:\n",
        "        continue\n",
        "    key = make_pair_key(a, b)\n",
        "    if key in pos_keys or key in out_keys:\n",
        "        continue\n",
        "\n",
        "\n",
        "    out.append({\"drugA_norm\": a, \"drugB_norm\": b, \"pair_key\": key, \"action_3class\": \"no_interaction\"})\n",
        "    out_keys.add(key)\n",
        "\n",
        "return pd.DataFrame(out)\n",
        "\n",
        "df_no = generate_no_interaction_pairs(3000)\n",
        "print(\"no_interaction:\", df_no.shape)\n",
        "\n",
        "df_model = pd.concat([df_pos, df_no], ignore_index=True).drop_duplicates(\"pair_key\")\n",
        "print(df_model[\"action_3class\"].value_counts())\n",
        "\n",
        "üî¥ TEXT KOLONUNU GARANTƒ∞ ALTINA AL\n",
        "\n",
        "df_model = df_model.copy()\n",
        "\n",
        "df_model[\"text\"] = (\n",
        "\"Interaction between \" +\n",
        "df_model[\"drugA_norm\"].astype(str) +\n",
        "\" and \" +\n",
        "df_model[\"drugB_norm\"].astype(str)\n",
        ")\n",
        "\n",
        "Label zaten vardƒ± ama garanti edelim\n",
        "\n",
        "label_names = [\"no_interaction\", \"monitor\", \"avoid\"]\n",
        "label2id = {n: i for i, n in enumerate(label_names)}\n",
        "df_model[\"label\"] = df_model[\"action_3class\"].map(label2id)\n",
        "\n",
        "Kontrol\n",
        "\n",
        "print(df_model[[\"text\", \"action_3class\", \"label\"]].head())\n",
        "print(df_model[\"action_3class\"].value_counts())\n",
        "\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "label_names = [\"no_interaction\",\"monitor\",\"avoid\"]\n",
        "label2id = {n:i for i,n in enumerate(label_names)}\n",
        "id2label = {i:n for n,i in label2id.items()}\n",
        "\n",
        "df_model[\"label\"] = df_model[\"action_3class\"].map(label2id)\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "df_model, test_size=0.3, random_state=42, stratify=df_model[\"label\"]\n",
        ")\n",
        "\n",
        "ds_train = Dataset.from_pandas(train_df[[\"text\",\"label\"]])\n",
        "ds_test  = Dataset.from_pandas(test_df[[\"text\",\"label\"]])\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "MODEL_BASE = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_BASE)\n",
        "\n",
        "def tok(batch):\n",
        "return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "ds_train = ds_train.map(tok, batched=True)\n",
        "ds_test  = ds_test.map(tok, batched=True)\n",
        "\n",
        "ds_train.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
        "ds_test.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "MODEL_BASE,\n",
        "num_labels=len(label_names),\n",
        "id2label=id2label,\n",
        "label2id=label2id\n",
        ")\n",
        "\n",
        "class weights (dengesizliƒüi azaltƒ±r)\n",
        "\n",
        "counts = train_df[\"label\"].value_counts().sort_index().values\n",
        "weights = (counts.sum() / (len(counts) * counts))\n",
        "class_weights = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "def compute_loss(self, model, inputs, return_outputs=False, kwargs):\n",
        "labels = inputs.get(\"labels\") if \"labels\" in inputs else inputs.get(\"label\")\n",
        "if labels is None:\n",
        "labels = inputs[\"label\"]\n",
        "outputs = model({k:v for k,v in inputs.items() if k in [\"input_ids\",\"attention_mask\"]})\n",
        "logits = outputs.logits\n",
        "loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
        "loss = loss_fct(logits, labels)\n",
        "return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "logits, labels = eval_pred\n",
        "preds = np.argmax(logits, axis=-1)\n",
        "return {\n",
        "\"accuracy\": accuracy_score(labels, preds),\n",
        "\"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
        "\"precision_macro\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
        "\"recall_macro\": recall_score(labels, preds, average=\"macro\"),\n",
        "}\n",
        "\n",
        "args = TrainingArguments(\n",
        "output_dir=\"drive/MyDrive/ƒ∞la√ß/content/biobert_ddi_3class_secondary2\",\n",
        "learning_rate=2e-5,\n",
        "per_device_train_batch_size=8,\n",
        "per_device_eval_batch_size=8,\n",
        "num_train_epochs=8,\n",
        "evaluation_strategy=\"epoch\",\n",
        "save_strategy=\"epoch\",\n",
        "logging_steps=50,\n",
        "report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "model=model,\n",
        "args=args,\n",
        "train_dataset=ds_train,\n",
        "eval_dataset=ds_test,\n",
        "tokenizer=tokenizer,\n",
        "compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer_model = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model_ddi = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "\n",
        "model_ddi.to(device)\n",
        "model_ddi.eval()\n",
        "\n",
        "print(\"‚úÖ Model ba≈üarƒ±yla y√ºklendi:\", MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "G2l8lamu6-Ox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}